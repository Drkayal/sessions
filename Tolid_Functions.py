# Tolid_Functions.py
# Ø¯ÙˆØ§Ù„ ØªÙˆÙ„ÙŠØ¯ ÙˆØ­ÙØ¸ Ø§Ù„Ø¬Ù„Ø³Ø§Øª ÙÙŠ PostgreSQL - Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ© ÙÙ‚Ø·
# ØªÙ… ÙØµÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ§Ù„ Ù…Ù† Tolid.py Ø§Ù„Ø£ØµÙ„ÙŠ

import os
import time
import random
import string
import math
import base64
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, wait, FIRST_COMPLETED
from datetime import datetime
import logging
from collections import deque
import threading
from io import StringIO
import psycopg2
from psycopg2 import sql, errors
import secrets

# ----------------------
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª PostgreSQL
# ----------------------
POSTGRES_CONFIG = {
    "host": os.getenv("PGHOST", "localhost"),
    "port": os.getenv("PGPORT", "5432"),
    "user": os.getenv("PGUSER", "postgres"),
    "password": os.getenv("PGPASSWORD", "your_password"),
    "database": os.getenv("PGDATABASE", "postgres")
}

# ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ PostgreSQL
POSTGRES_PERFORMANCE_SETTINGS = {
    "shared_buffers": "1GB",  # 25% Ù…Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡Ø§
    "work_mem": "16MB",       # Ø°Ø§ÙƒØ±Ø© Ù„ÙƒÙ„ Ø¹Ù…Ù„ÙŠØ© ØªØ±ØªÙŠØ¨/Ù‡Ø§Ø´
    "maintenance_work_mem": "256MB",  # Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ©
    "checkpoint_timeout": "30min",    # ØªÙ‚Ù„ÙŠÙ„ ØªÙƒØ±Ø§Ø± Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙØªÙŠØ´
    "max_wal_size": "2GB",            # Ø­Ø¬Ù… Ø£ÙƒØ¨Ø± Ù„Ù…Ù„ÙØ§Øª WAL
    "min_wal_size": "1GB",
    "checkpoint_completion_target": "0.9",
    "random_page_cost": "1.1",        # Ù…ÙÙŠØ¯ Ù„Ù„Ø³ÙŠØ±ÙØ±Ø§Øª SSD
    "effective_cache_size": "3GB",    # ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
}

# ----------------------
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯ ÙˆØ§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬
# ----------------------
PREFIX = "1BJWap1sAU"           # Ø¨Ø§Ø¯Ø¦Ø© Ø«Ø§Ø¨ØªØ© (10 Ø­Ø±ÙˆÙ)
TOTAL_LENGTH = 344              # Ø·ÙˆÙ„ Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„
END_CHAR = "="                  # ÙŠØ¬Ø¨ Ø£Ù† ØªÙ†ØªÙ‡ÙŠ Ø¨Ù‡
MIDDLE_LEN = TOTAL_LENGTH - len(PREFIX) - len(END_CHAR)  # Ø·ÙˆÙ„ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ
ALLOWED_CHARS = string.ascii_letters + string.digits + "-_"  # Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
DEFAULT_BATCH = int(os.getenv("DEFAULT_BATCH", "50000"))           # Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ (Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙ‡ÙŠØ¦Ø©)
MAX_ALLOWED = 10_000_000_000    # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 10 Ù…Ù„ÙŠØ§Ø± Ø³Ø¬Ù„
MAX_PENDING_BATCHES = int(os.getenv("MAX_PENDING_BATCHES", "100"))       # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø¯ÙØ¹Ø§Øª Ø§Ù„Ù…Ø¹Ù„Ù‚Ø© (Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙ‡ÙŠØ¦Ø©)
PARTITION_SIZE = 100_000_000    # Ø­Ø¬Ù… ÙƒÙ„ partition (Ù„Ù… ÙŠØ¹Ø¯ Ù…Ø³ØªØ®Ø¯Ù…Ø§Ù‹ Ø¥Ø°Ø§ ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… HASH)
HASH_PARTITIONS = int(os.getenv("HASH_PARTITIONS", "64"))  # Ø¹Ø¯Ø¯ Ø£Ù‚Ø³Ø§Ù… HASH Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…
MAX_WORKERS = int(os.getenv("MAX_WORKERS", str(max(1, mp.cpu_count() * 2))))  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¶Ø¹Ù Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù†ÙˆÙŠØ© Ø£Ùˆ Ù…Ù† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦ÙŠØ©
CHUNK_SIZE = 10000             # Ø­Ø¬Ù… chunk Ù„Ù„Ø¥Ø¯Ø®Ø§Ù„

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø­Ø§Ù„Ø©
performance_stats = {
    'total_generated': 0,
    'total_time': 0.0,
    'avg_speed': 0.0
}
task_history = deque(maxlen=200)
stats_lock = threading.Lock()

# ----------------------
# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© PostgreSQL
# ----------------------
def create_postgres_connection(dbname=None):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª PostgreSQL"""
    config = POSTGRES_CONFIG.copy()
    if dbname:
        config["database"] = dbname
    conn = psycopg2.connect(**config)
    
    # ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø§ØªØµØ§Ù„
    conn.autocommit = False
    cur = conn.cursor()
    try:
        cur.execute("SET work_mem = '16MB'")
        cur.execute("SET maintenance_work_mem = '256MB'")
        cur.execute("SET random_page_cost = 1.1")
    except:
        pass
    cur.close()
    
    return conn

def create_database(dbname):
    """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    try:
        conn = create_postgres_connection()
        conn.autocommit = True
        cur = conn.cursor()
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        cur.execute("SELECT 1 FROM pg_database WHERE datname = %s", (dbname,))
        exists = cur.fetchone()
        
        if not exists:
            cur.execute(sql.SQL("CREATE DATABASE {} WITH ENCODING 'UTF8'").format(sql.Identifier(dbname)))
            logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {dbname}")
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            conn_db = create_postgres_connection(dbname)
            conn_db.autocommit = True
            cur_db = conn_db.cursor()
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
            for setting, value in POSTGRES_PERFORMANCE_SETTINGS.items():
                try:
                    cur_db.execute(f"ALTER DATABASE {dbname} SET {setting} = '{value}'")
                except Exception as e:
                    logger.warning(f"ØªØ¹Ø°Ø± ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø¹Ø¯Ø§Ø¯ {setting}: {e}")
            
            cur_db.close()
            conn_db.close()
        
        cur.close()
        conn.close()
        return True
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        return False

def create_table(dbname, tablename):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„ØªÙ‚Ø³ÙŠÙ…"""
    try:
        conn = create_postgres_connection(dbname)
        conn.autocommit = True
        cur = conn.cursor()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù…Ø¹ HASH partitioning
        create_table_sql = sql.SQL("""
            CREATE TABLE IF NOT EXISTS {} (
                id BIGSERIAL,
                session_code TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            ) PARTITION BY HASH (session_code)
        """).format(sql.Identifier(tablename))
        
        cur.execute(create_table_sql)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… (partitions)
        for i in range(HASH_PARTITIONS):
            partition_name = f"{tablename}_part_{i}"
            try:
                partition_sql = sql.SQL("""
                    CREATE TABLE IF NOT EXISTS {} PARTITION OF {} 
                    FOR VALUES WITH (MODULUS {}, REMAINDER {})
                """).format(
                    sql.Identifier(partition_name),
                    sql.Identifier(tablename),
                    sql.Literal(HASH_PARTITIONS),
                    sql.Literal(i)
                )
                cur.execute(partition_sql)
            except errors.DuplicateTable:
                pass  # Ø§Ù„Ù‚Ø³Ù… Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„
        
        # Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ ÙØ±ÙŠØ¯ Ø¹Ù„Ù‰ session_code
        try:
            index_sql = sql.SQL("CREATE UNIQUE INDEX IF NOT EXISTS {} ON {} (session_code)").format(
                sql.Identifier(f"idx_{tablename}_session_code"),
                sql.Identifier(tablename)
            )
            cur.execute(index_sql)
        except Exception as e:
            logger.warning(f"ØªØ¹Ø°Ø± Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³: {e}")
        
        cur.close()
        conn.close()
        logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙˆÙ„ {tablename} Ù…Ø¹ {HASH_PARTITIONS} Ù‚Ø³Ù…")
        return True
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙˆÙ„: {e}")
        return False

def get_row_count(dbname, tablename):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„"""
    try:
        conn = create_postgres_connection(dbname)
        cur = conn.cursor()
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø³Ø±ÙŠØ¹ Ù„Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
        cur.execute(sql.SQL("""
            SELECT 
                CASE 
                    WHEN reltuples > 1000000 THEN reltuples::bigint
                    ELSE (SELECT COUNT(*) FROM {})
                END as estimated_count
            FROM pg_class 
            WHERE relname = %s
        """).format(sql.Identifier(tablename)), (tablename,))
        
        result = cur.fetchone()
        count = result[0] if result else 0
        
        cur.close()
        conn.close()
        return int(count)
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ØµÙÙˆÙ: {e}")
        return 0

# ----------------------
# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙˆÙ„ÙŠØ¯
# ----------------------
def gen_one():
    """ØªÙˆÙ„ÙŠØ¯ Ø¬Ù„Ø³Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©"""
    s = ''.join(random.choices(ALLOWED_CHARS, k=MIDDLE_LEN + 10))
    middle_part = s[:MIDDLE_LEN]
    return PREFIX + middle_part + END_CHAR

def generate_batch(batch_size):
    """ØªÙˆÙ„ÙŠØ¯ Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø¨Ø³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ© Ø¯ÙˆÙ† ÙØ­Øµ ØªÙƒØ±Ø§Ø± Ù…Ø­Ù„ÙŠ (Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ UNIQUE ÙÙŠ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©)."""
    return [gen_one() for _ in range(batch_size)]

def progress_bar(percentage, length=20):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø´Ø±ÙŠØ· ØªÙ‚Ø¯Ù… Ù†ØµÙŠ"""
    filled = int(length * percentage / 100)
    empty = length - filled
    return f"[{'â–ˆ' * filled}{'â–‘' * empty}] {percentage:.1f}%"

def log_activity(action, details=""):
    """ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†Ø´Ø§Ø· ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙÙ‚Ø·"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_message = f"{timestamp} - {action} - {details}"
    logger.info(log_message)
    task_history.append(log_message)

def monitor_performance(delta_inserted, delta_time):
    """ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¡Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù„Ø­Ø¸ÙŠØ© Ø®Ù„Ø§Ù„ Ø§Ù„ÙØªØ±Ø©"""
    elapsed = max(1e-6, delta_time)
    speed = delta_inserted / elapsed

    with stats_lock:
        performance_stats['total_generated'] += delta_inserted
        performance_stats['total_time'] += elapsed
        if performance_stats['total_time'] > 0:
            performance_stats['avg_speed'] = performance_stats['total_generated'] / performance_stats['total_time']

    return speed, elapsed

def bulk_insert_sessions(dbname, tablename, sessions):
    """Ø¥Ø¯Ø®Ø§Ù„ Ø¯ÙØ¹Ø§Øª ÙƒØ¨ÙŠØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… COPY FROM STDIN Ù„Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£Ù…Ø«Ù„"""
    if not sessions:
        return 0
    
    conn = None
    try:
        conn = create_postgres_connection(dbname)
        cur = conn.cursor()
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… COPY Ù„Ø£Ø¯Ø§Ø¡ Ø£ÙØ¶Ù„
        f = StringIO()
        for session in sessions:
            f.write(f"{session}\n")
        
        f.seek(0)
        cur.copy_expert(
            sql.SQL("COPY {} (session_code) FROM STDIN WITH CSV").format(
                sql.Identifier(tablename)
            ), 
            f
        )
        
        conn.commit()
        inserted = len(sessions)
        
        cur.close()
        conn.close()
        return inserted
        
    except errors.UniqueViolation:
        # ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ ØªÙƒØ±Ø§Ø±ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ù…Ø¹ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªÙƒØ±Ø§Ø±
        try:
            if conn:
                conn.rollback()
        except Exception:
            pass
        return safe_bulk_insert(dbname, tablename, sessions)
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠ: {e}")
        try:
            if conn:
                conn.rollback()
        except Exception:
            pass
        return safe_bulk_insert(dbname, tablename, sessions)

def safe_bulk_insert(dbname, tablename, sessions):
    """Ø¥Ø¯Ø®Ø§Ù„ Ø¢Ù…Ù† Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª"""
    if not sessions:
        return 0
    
    try:
        conn = create_postgres_connection(dbname)
        cur = conn.cursor()
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø¥Ù„Ù‰ chunks
        inserted = 0
        for i in range(0, len(sessions), CHUNK_SIZE):
            chunk = sessions[i:i + CHUNK_SIZE]
            
            cur.execute(
                sql.SQL("""
                    INSERT INTO {} (session_code) 
                    SELECT * FROM UNNEST(%s::text[]) 
                    ON CONFLICT (session_code) DO NOTHING
                """).format(sql.Identifier(tablename)),
                [chunk]
            )
            inserted += cur.rowcount
        
        conn.commit()
        cur.close()
        conn.close()
        return inserted
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ø¢Ù…Ù†: {e}")
        return 0

def generate_parallel(target_count, dbname, tablename, chat_id, context, progress_message_id, cancel_event):
    """Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ù…Ø¹ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…"""
    start_time = time.time()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ø¬Ø¯ÙˆÙ„ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙˆÙ†Ø§ Ù…ÙˆØ¬ÙˆØ¯ÙŠÙ†
    if not create_database(dbname):
        try:
            context.bot.edit_message_text(chat_id=chat_id, message_id=progress_message_id, text="Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        except Exception:
            pass
        return 0
    
    if not create_table(dbname, tablename):
        try:
            context.bot.edit_message_text(chat_id=chat_id, message_id=progress_message_id, text="Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙˆÙ„")
        except Exception:
            pass
        return 0

    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ
    existing_count = get_row_count(dbname, tablename)
    total_inserted = 0
    
    if existing_count >= target_count:
        try:
            context.bot.edit_message_text(
                chat_id=chat_id,
                message_id=progress_message_id,
                text=f"Ø§Ù„Ø¬Ø¯ÙˆÙ„ ÙŠØ­ØªÙˆÙŠ Ø¨Ø§Ù„ÙØ¹Ù„ Ø¹Ù„Ù‰ {existing_count} Ø¬Ù„Ø³Ø© (>= Ø§Ù„Ù…Ø·Ù„ÙˆØ¨)."
            )
        except Exception:
            pass
        return existing_count

    remaining = target_count - existing_count
    batch_size = min(DEFAULT_BATCH, max(10000, remaining // 100))
    last_report_time = time.time()
    last_report_inserted = 0

    # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ù„Ù„Ø¯ÙØ¹Ø§Øª Ø§Ù„Ù…Ø¹Ù„Ù‚Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    dynamic_max_pending = min(MAX_PENDING_BATCHES, MAX_WORKERS * 2)

    try:
        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
            pending = set()
            submitted = 0
            total_batches = (remaining + batch_size - 1) // batch_size

            # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¯ÙØ¹Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
            for _ in range(min(dynamic_max_pending, total_batches)):
                if submitted >= total_batches:
                    break
                pending.add(executor.submit(generate_batch, batch_size))
                submitted += 1

            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø§Øª
            while (pending or submitted < total_batches) and not cancel_event.is_set():
                if not pending and submitted < total_batches:
                    for _ in range(min(dynamic_max_pending, total_batches - submitted)):
                        pending.add(executor.submit(generate_batch, batch_size))
                        submitted += 1

                if not pending:
                    break

                done, _ = wait(pending, return_when=FIRST_COMPLETED, timeout=30)
                for fut in done:
                    pending.discard(fut)
                    try:
                        batch = fut.result(timeout=30)
                        if batch:
                            inserted = bulk_insert_sessions(dbname, tablename, batch)
                            total_inserted += inserted

                            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù… ÙƒÙ„ 60 Ø«Ø§Ù†ÙŠØ© ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø±Ø³Ø§Ù„Ø©
                            now = time.time()
                            should_report = (now - last_report_time >= 60) or (existing_count + total_inserted >= target_count)
                            if should_report:
                                current_total = existing_count + total_inserted
                                perc = min(100.0, (current_total / target_count) * 100)
                                delta_inserted = total_inserted - last_report_inserted
                                delta_time = now - last_report_time
                                speed, _ = monitor_performance(delta_inserted, delta_time)
                                try:
                                    context.bot.edit_message_text(
                                        chat_id=chat_id,
                                        message_id=progress_message_id,
                                        text=(
                                            f"{progress_bar(perc)}\n"
                                            f"Ø§Ù„ØªÙ‚Ø¯Ù…: {current_total}/{target_count}\n"
                                            f"Ø§Ù„Ø³Ø±Ø¹Ø©: {speed:.0f} Ø¬Ù„Ø³Ø©/Ø«Ø§Ù†ÙŠØ©\n"
                                            f"Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ù†Ù‚Ø¶ÙŠ: {int(now - start_time)} Ø«Ø§Ù†ÙŠØ©"
                                        ),
                                        parse_mode="Markdown"
                                    )
                                except Exception as e:
                                    logger.debug(f"ØªØ¹Ø°Ø± ØªØ­Ø¯ÙŠØ« Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªÙ‚Ø¯Ù…: {e}")

                                last_report_time = now
                                last_report_inserted = total_inserted

                    except Exception as e:
                        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø©: {e}")

    except Exception as e:
        logger.exception(f"Ø®Ø·Ø£ ÙÙŠ generate_parallel: {e}")
        try:
            context.bot.edit_message_text(chat_id=chat_id, message_id=progress_message_id, text=f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {e}")
        except Exception:
            pass

    finally:
        # Ø§Ù„Ø¥Ø¨Ù„Ø§Øº Ø¹Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¹Ø¨Ø± ØªØ¹Ø¯ÙŠÙ„ Ù†ÙØ³ Ø§Ù„Ø±Ø³Ø§Ù„Ø©
        final_count = existing_count + total_inserted
        total_time = time.time() - start_time

        try:
            if cancel_event.is_set():
                context.bot.edit_message_text(
                    chat_id=chat_id,
                    message_id=progress_message_id,
                    text=f"ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©. ØªÙ… Ø­ÙØ¸ {final_count} Ø¬Ù„Ø³Ø©.")
                log_activity("CANCELLED", f"Ø£ÙˆÙ‚Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø¹Ø¯ ØªÙˆÙ„ÙŠØ¯ {total_inserted} Ø¬Ù„Ø³Ø©")
            else:
                context.bot.edit_message_text(
                    chat_id=chat_id,
                    message_id=progress_message_id,
                    text=(
                        f"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªÙˆÙ„ÙŠØ¯!\n\n"
                        f"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬:\n"
                        f"â€¢ Ø§Ù„Ù‡Ø¯Ù: {target_count} Ø¬Ù„Ø³Ø©\n"
                        f"â€¢ Ø§Ù„Ù…Ø­ÙÙˆØ¸: {final_count} Ø¬Ù„Ø³Ø©\n"
                        f"â€¢ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {int(total_time)} Ø«Ø§Ù†ÙŠØ©\n"
                        f"â€¢ Ø§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù…ØªÙˆØ³Ø·Ø©: {final_count/total_time:.0f} Ø¬Ù„Ø³Ø©/Ø«Ø§Ù†ÙŠØ©"
                    )
                )
                log_activity("COMPLETED", f"Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {final_count} Ø¬Ù„Ø³Ø© ÙÙŠ {total_time:.1f} Ø«Ø§Ù†ÙŠØ©")
        except Exception:
            pass

        return final_count

def alert_check(context):
    """ÙØ­Øµ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ù„Ù„Ù…Ù‡Ø§Ù… Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰"""
    # Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ active_tasks Ù…Ù† Ø§Ù„Ø¨ÙˆØª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    # Ø³ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡Ø§ Ù…Ù† Ø§Ù„Ø¨ÙˆØª Ø§Ù„Ù…ÙˆØ­Ø¯
    pass

# Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙˆØ§Ù„Ø­Ø§Ù„Ø©
def get_performance_stats():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    with stats_lock:
        return performance_stats.copy()

def get_task_history():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ù‡Ø§Ù…"""
    return list(task_history)

def reset_performance_stats():
    """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    with stats_lock:
        performance_stats['total_generated'] = 0
        performance_stats['total_time'] = 0.0
        performance_stats['avg_speed'] = 0.0